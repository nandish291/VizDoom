{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vizdoom as vzd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools as it\n",
    "import skimage.transform\n",
    "\n",
    "import os\n",
    "from time import sleep, time\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, available_actions_count):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.convolutional_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.full_connected_layers = nn.Sequential(\n",
    "            nn.Linear(192, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(64, available_actions_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional_layers(x)\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.full_connected_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent:\n",
    "    def __init__(self, action_size, params):\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.discount = params['discount_factor']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.epsilon_decay = params['epsilon_decay']\n",
    "        self.epsilon_min = params['epsilon_min']\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.device = params['device']\n",
    "        self.episode_with_rewards = deque(maxlen=params['replay_buffer_size'])\n",
    "        self.dictionary = dict()\n",
    "\n",
    "        self.behaviour_policy_net = NeuralNet(action_size).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.SGD(self.behaviour_policy_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            state = self.array_to_tensor(state)\n",
    "            q_values = self.behaviour_policy_net(state)\n",
    "            action = q_values.max(dim=1)[1].item()\n",
    "            return action\n",
    "\n",
    "    def array_to_tensor(self, arr):\n",
    "        arr = np.expand_dims(arr, axis=0)\n",
    "        arr = torch.from_numpy(arr).float().to(self.device)\n",
    "        return arr\n",
    "\n",
    "    def update_behaviour_policy_net(self, replay_buffer):\n",
    "        batch = replay_buffer.copy()\n",
    "        episode = replay_buffer.copy()\n",
    "        batch = np.array(batch, dtype=object)\n",
    "\n",
    "        states = np.stack(batch[:, 0]).astype(float)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2].astype(float)\n",
    "        dones = batch[:, 3].astype(bool)\n",
    "        not_dones = ~dones\n",
    "        returns = np.zeros((dones.shape[0]))\n",
    "        \n",
    "        g = 0\n",
    "        for i in reversed(range(len(episode))):\n",
    "            count = self.dictionary.get(states[i].tobytes())\n",
    "            if count != None:\n",
    "                self.dictionary.update({states[i].tobytes(): count + 1})\n",
    "            else:\n",
    "                self.dictionary[states[i].tobytes()] = 1\n",
    "            g = g * self.discount + rewards[i]\n",
    "            returns[i] = g / self.dictionary.get(states[i].tobytes())\n",
    "\n",
    "        for i in range(len(episode)):\n",
    "            self.episode_with_rewards.append((states[i], actions[i], returns[i]))\n",
    "\n",
    "        sample = random.sample(self.episode_with_rewards, self.batch_size)    \n",
    "        sample = np.array(sample, dtype=object)\n",
    "\n",
    "        sample_actions = sample[:, 1].astype(int)\n",
    "        sample_returns = sample[:, 2].astype(int)\n",
    "        sample_states = np.stack(sample[:, 0]).astype(float)\n",
    "\n",
    "        row_idx = np.arange(self.batch_size) \n",
    "\n",
    "        q_targets = sample_returns.copy()\n",
    "        q_targets = torch.from_numpy(q_targets).float().to(self.device)\n",
    "\n",
    "        idx = row_idx, sample_actions\n",
    "        sample_states = torch.from_numpy(sample_states).float().to(self.device)\n",
    "        action_values = self.behaviour_policy_net.forward(sample_states)[idx].float().to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        error = self.loss_fn(q_targets, action_values)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        return error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, resolution):\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "def visualize(game, agent, params):\n",
    "    for _ in range(params['episodes_to_watch']):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer, params['resolution'])\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.set_action(params['actions'][best_action_index])\n",
    "            for _ in range(params['frame_repeat']):\n",
    "                game.advance_action()\n",
    "\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mc_agent(game, train_parameters):\n",
    "    actions = train_parameters['actions']\n",
    "\n",
    "    # Initialize our agent with the set parameters\n",
    "    agent = MCAgent(len(actions), train_parameters)\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    time_steps = train_parameters['total_time_steps']\n",
    "    replay_buffer = deque(maxlen=train_parameters['replay_buffer_size'])\n",
    "\n",
    "    game.new_episode()\n",
    "    train_scores = []\n",
    "    train_loss = []\n",
    "    global_step = 0\n",
    "\n",
    "    for _ in trange(time_steps):\n",
    "        state = preprocess(game.get_state().screen_buffer, train_parameters['resolution'])\n",
    "        action = agent.get_action(state)\n",
    "        reward = game.make_action(actions[action], train_parameters['frame_repeat'])\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        replay_buffer.append((state, action, reward, done))\n",
    "\n",
    "        if global_step > agent.batch_size and done:\n",
    "            train_loss.append(agent.update_behaviour_policy_net(replay_buffer))\n",
    "            replay_buffer.clear()\n",
    "\n",
    "        if done:\n",
    "            train_scores.append(game.get_total_reward())\n",
    "            game.new_episode()\n",
    "\n",
    "        global_step += 1\n",
    "    visualize(game, agent, train_parameters)\n",
    "    train_scores = np.array(train_scores)\n",
    "    print(\"Results: mean: %.1f +/- %.1f,\" % (train_scores.mean(), train_scores.std()),\n",
    "            \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())\n",
    "\n",
    "    game.close()\n",
    "    return agent, game, train_scores, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(os.path.join(vzd.scenarios_path, \"basic.cfg\"))\n",
    "    game.set_window_visible(False)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "    train_parameters = {\n",
    "        'learning_rate': 0.00025,\n",
    "        'discount_factor': 0.99,\n",
    "        'total_time_steps': 500000,\n",
    "        'replay_buffer_size': 10000,\n",
    "        'batch_size': 64,\n",
    "        'frame_repeat': 12,\n",
    "        'resolution': (30, 45),\n",
    "        'episodes_to_watch': 5,\n",
    "        'device': torch.device('cuda'),\n",
    "        'epsilon': 1,\n",
    "        'epsilon_decay': 0.9996,\n",
    "        'epsilon_min': 0.1,\n",
    "        'actions': actions\n",
    "    }\n",
    "\n",
    "    agent, game, returns, loss = train_mc_agent(game, train_parameters)\n",
    "\n",
    "    game.close()\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "    game.init()\n",
    "\n",
    "    visualize(game, agent, train_parameters)\n",
    "    game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array([returns])], ['monte carlo'], ['b'], 'episodes', 'Vizdoom Deep-Monte Carlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.set_window_visible(True)\n",
    "game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "game.init()\n",
    "\n",
    "visualize(game, agent, train_parameters)\n",
    "game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
